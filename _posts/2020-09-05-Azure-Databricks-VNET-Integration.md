
# Azure Databricks Networking Deep dive

This is an updated version of my article at [Medium.com](https://medium.com/cloudsafari-ca/azure-databricks-deployments-issues-3195ea8c7f56) originally written on December 2019 as some changes happened since then 

Azure Databricks  [workspace](https://docs.microsoft.com/en-us/azure/databricks/workspace/) is a code authoring and collaboration workspace that can have one or more Apache Spark clusters. So as a prerequisites to create the cluster, there has to be a  [Virtual Network (VNET)](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-networks-overview)  to have the machines attached to itd. That’s why in the managed resource group, if you don’t choose to use your own VNET, there will be a VNET created for you.

Many organizations have restrictions about the VNET creation and prefer to integrate the Databricks clusters into their network infrastructure, that’s why Azure Databricks now supports VNET injection. VNET injection enable you to use an existing VNET for hosting the Databricks clusters.

The  [docs](https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject)  are listing the benefits & the requirements of these VNETs so I won’t list all of them here however the most important ones

-   Have to be in the same subscription
-   **Address space:**  A CIDR block between /16 — /24 for the virtual network and a CIDR block up to /26 for the private and public subnets
-   Two subnet per workspace
-   Can’t use the same subnet by two workspaces --> That means every time there's a new workspace, there's quite a checklist to go through which makes the idea of hosting multiple projects/departments on the same workspace a favorable idea but that's the topic of another article. 
- ----------------------------------------------
**But first, what’s deployed inside the customer’s VNET? is Azure Databricks entirely deployed there?**
No, the control plane and web UI are always deployed in a Microsoft-managed subscription. That’s why the Azure Databrisk UI is always https://< Azure Region >.azuredatabricks.net

![Azure Databricks Network Architecture](/assets/images/posts/2020/dbricks-vnet-architecture.png)

During workspace deployment, there’s no clusters created yet. So during deployment, Databricks would insure that the minimum requirements for the clusters to be successfully deployed are met.

**What happens during deployment time?**

![Azure Databricks Creation Flowchart](/assets/images/posts/2020/dbricks-creation-flowchart-network.png)

-   Delegation to Microsoft.Databricks/workspaces is made on each subnet. This delegation will be used by the service to create a Network Intent Policy. Azure Network Intent Policy is a feature used by some Microsoft first party providers like SQL Managed Instance. It’s not available for customers’ use as there’s no public docs for it. In the case of Databricks, it’s used to maintain the NSG rules added by the workspace creation. If you try to delete one of the rules, you will get an error message because of the Intent Policy.
-   Check if there’s already an NSG attached to the subnet. If not then the creation of the workspace will fail. In the portal experience, the ARM template generated by the portal is avoiding the failure by creating an NSG and attach it. It will create only one NSG for both private & public subnets. The rules are identical anyways even if you provided two NSGs, you will find the same set of rules.
-   If there’s NSG attached, new rules will be added and protected by the Intent Policy so it can’t be changed or deleted.

I’ve created  [a sample ARM template on github](https://github.com/mosharafMS/ARM-Templates/tree/master/Databricks/vnetInjection)  that deploys Databricks workspace with VNET integration but you have to setup the delegation and attache NSGs to the subnet before you deploy the template

-----------------------------------------

**Why having two subnets?**
Each machine in the Databricks cluster has two virtual network cards (NIC), one with private IP only attached to the private subnet and one with both private & public IPs attached to public subnets. The public IPs used to communicate between your cluster and the control plane of Azure Databricks plus any other data sources that might reside outside your Vnet. All the inter-cluster communication happens on the
 **Are the subnets exclusively used by the workspace clusters only?**
No, you can use subnets that has NICs already attached to them. But you can't have two different workspaces on the same subnet

## Integrating Azure Databricks workspaces with firewalls
On of the main reasons to have Azure Databricks workspaces integrated with VNET is to utilize your existing firewalls. The workflow to do that is

 1. Create and assign Route Tables that  will  force the traffic from
    Azure Databricks to be filtered by the firewall first.
 2. In the firewall you add rules to allow the traffic needed for your workspace

> The traffic coming to the public IPs of the workspace clusters,
> doesn’t pass on the firwall and if your routing route all 0.0.0.0/0 to
> the firewall that means the firewall will only see the return of the
> traffic and for any stateful firewall like Azure Firewall, it will
> drop the traffic. You have to add exception of the routing for the
> control plane. The screenshot below shows this problem. For a complete
> list of the IPs per region, refer to the [docs](https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/udr)

![Asymmetric traffic](/assets/images/posts/2020/assymetric-network.png)

Because of how stateful firewalls work , the routing table should avoid routing *ALL* the traffic but to exclude the traffic going to the control plane and the web app of Databricks. Per each Azure region that has Databricks enabled in, there are two IP ranges, one for the control plane and one for the webapp. These are unique to the region and should be excluded from the routing. The screenshot below shows the routing for a databricks in Canada Central region. 
 
![Sample Routing Table](/assets/images/posts/2020/routing-table.png)

In my testing, I used [Azure Firewall](https://docs.microsoft.com/en-us/azure/firewall/overview) and I'll list all the Network Rules and Application Rules I added to have a successful cluster creation and running sample notebook. 

## Documented Rules
### DBFS root Blob storage IP address
Each workspace has a storage account created and managed by Databricks to act as the local file system of the clusters of this workspace. Also is used to save the notebooks. There's a storage account per workspace in the managed resource group of the workspace. The naming convention of it is dbstorage < random alphanumeric string> 
Until now, Databricks doesn't use private endpoint for these storage accounts (something for the near future may be) so the traffic destined to this storage account has to pass through the firewall and you need to use Application Rule (by using the FQDN) to allow it. 
I highly recommended not using something like `*.blob.core.windows.net` because that allows the Databricks cluster to reach any storage account on Azure anywhere in the globe which will open the door for exfilteration attacks. 

###  Metastore, artifact Blob storage, log Blob storage, and Event Hub endpoint IP addresses
**Metastore** is a mysql database where the metadata about the workspace is saved
. Which means if you are using IP & port combination then you need to get the IP of the URL from the docs and use port 3306. For example for Canada Central 

    nslookup consolidated-canadacentral-prod-metastore.mysql.database.azure.com

    Name:    cr2.canadacentral1-a.control.database.windows.net
    Address:  52.228.35.221
    Aliases:  consolidated-canadacentral-prod-metastore.mysql.database.azure.com
In Azure Firewall, we can use the FQDN in the network rules so there's no need to get the IP but if your firewall doesn't support that then get the IP and periodically update it.   
**Artifact Blob storage** both primary & secondary storage accounts (which can be the same account in some regions) are the storage account where the scripts and the binary files of Databricks are saved at. 

**Log Blob storage** is another storage used for cluster logs 

**Event Hub endpoint** eventhub endpoint used for shipping logs as well. This is eventhub kafka so it uses port 9093. For more details, refer to the [docs](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-faq#what-ports-do-i-need-to-open-on-the-firewall)


## Special un-documented domains
During my testing with Azure Firewall & Databricks, I found that the docs didn’t cover all the FQDNs that are requested by my cluster. From my testing I found out these extra ones
-   Ubuntu updates → used for ubuntu updates since Databricks is built on ubuntu image ==> enabled *.ubuntu.com
-   snap packages → *.snapcraft.io
-   terracotta → *.terracotta.org
-   cloudflare → *.cloudflare.com
-  ICMP Type 8 (Ping) to 172.217.13.164 which is an IP belongs to Google. I didn't see any docs for it but since it's just ping, I didn't see an issue with that traffic. It can be a heartbeat to check if the server has connectivity with the internet. It can be due to exceptions raised by blocked URLs so the servers check connectivity. 
- 
- 
zrdfepirv2yto21prdstr02a.blob.core.windows.net	
zrdfepirv2yt1prdstr06a.blob.core.windows.net	
sts.amazonaws.com
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0NjUzNjMyNjIsNjUyNzQwMjc0LC0xMz
Y5MTc5Mzk3LDE2MzU0MDgxODUsLTg5ODczNDI1MiwtODYzNzM1
Njc1LC00MDI2MDYxMzAsMTkwNjgwMzY5OCwxNzA1NjUyNDc2LD
U5MzY5MzI4LC0xNDE5NDI4OTYzLDg0OTkwNzUxMiwxODU2NzE1
NzcsMjAwNTM1Mzk5Miw0MTk0MzkwNywtODQ1MjcxNzQ2LC0xNj
k3NTcwODc0XX0=
-->